{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert-finetune.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TrYkWIiSTOK"
      },
      "source": [
        "# Finetuning of german-sentiment-bert\n",
        "\n",
        "To be executed in Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH1Z9g1qJk-0"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0XI7YjFJcf2"
      },
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_IK0aEPJuG4"
      },
      "source": [
        "class SentiCSVDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Custom dataset class for sentiment analysis data in a CSV file.\n",
        "    \n",
        "    Tailored towards the pretrained model oliverguhr/german-sentiment-bert.\"\"\"\n",
        "    def __init__(self, csv_path):\n",
        "        self.csv_path = csv_path\n",
        "\n",
        "        raw_texts = []\n",
        "        raw_labels = []\n",
        "        if csv_path is not None:\n",
        "            with open(os.path.expanduser(csv_path), 'r') as f:\n",
        "                reader = csv.reader(f, delimiter=',')\n",
        "                for row in reader:\n",
        "                    if len(row) != 2:\n",
        "                        raise ValueError('Invalid row encountered.')\n",
        "                    raw_texts.append(row[0])\n",
        "                    raw_labels.append(int(row[1]))\n",
        "        else:  # Default data for testing\n",
        "            raw_texts = [\n",
        "                'Du hirnloser Vollidiot!', 'Ich mag dich sehr.', 'Alles hat ein Ende.', 'Nur die Wurst hat zwei.',\n",
        "                'So ist das Leben.', 'Der zu frühe Vogel muss auf den Wurm warten.', 'Was für eine Katastrophe.'\n",
        "            ]\n",
        "            raw_labels = [1, 0, 2, 2, 2, 2, 1]\n",
        "       \n",
        "        self.raw_texts = raw_texts\n",
        "        self.raw_labels = raw_labels\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('oliverguhr/german-sentiment-bert')\n",
        "        self.encodings = self.tokenizer(self.raw_texts, return_tensors='pt', truncation=True, padding=True)\n",
        "        self.input_ids = self.encodings['input_ids']\n",
        "\n",
        "        self.labels = torch.tensor(self.raw_labels, dtype=torch.int64)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.raw_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpTg29rrJwK5"
      },
      "source": [
        "train_dataset = SentiCSVDataset('')\n",
        "\n",
        "eval_dataset = SentiCSVDataset('')\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained('oliverguhr/german-sentiment-bert')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15BIHiC9J0D7"
      },
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    learning_rate=1e-5,\n",
        "    num_train_epochs=100,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwDJ0q8pJ3s6"
      },
      "source": [
        "trainer.train()\n",
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snIi61cBKLyv"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NczRoRT8LULf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}